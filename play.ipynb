{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Starting Iteration 1/3\n",
      "\n",
      "Starting Episode 1/5000\n",
      "  Trajectory 1: Success! Steps = 231, Final Reward = 1057.52\n",
      "  Trajectory 2: Success! Steps = 214, Final Reward = 1055.43\n",
      "  Trajectory 3: Success! Steps = 790, Final Reward = 1202.52\n",
      "  Trajectory 4: Success! Steps = 367, Final Reward = 1095.95\n",
      "  Trajectory 5: Success! Steps = 880, Final Reward = 1227.65\n",
      "  Trajectory 6: Success! Steps = 737, Final Reward = 1187.62\n",
      "  Trajectory 7: Success! Steps = 837, Final Reward = 1215.10\n",
      "  Trajectory 8: Success! Steps = 302, Final Reward = 1079.59\n",
      "  Trajectory 9: Success! Steps = 695, Final Reward = 1174.27\n",
      "  Trajectory 10: Success! Steps = 440, Final Reward = 1113.22\n",
      "  Trajectory 11: Success! Steps = 450, Final Reward = 1116.79\n",
      "  Trajectory 12: Success! Steps = 758, Final Reward = 1187.62\n",
      "  Trajectory 13: Success! Steps = 606, Final Reward = 1157.18\n",
      "  Trajectory 14: Success! Steps = 71, Final Reward = 1017.27\n",
      "  Trajectory 15: Success! Steps = 684, Final Reward = 1177.39\n",
      "  Trajectory 16: Success! Steps = 638, Final Reward = 1158.23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 425\u001b[0m\n\u001b[1;32m    422\u001b[0m maze_env \u001b[38;5;241m=\u001b[39m Maze(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Start with smallest maze\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_maze_grpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaze_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Save the trained policy\u001b[39;00m\n\u001b[1;32m    428\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(policy\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaze_policy.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 254\u001b[0m, in \u001b[0;36mtrain_maze_grpo\u001b[0;34m(maze_env, num_episodes, group_size)\u001b[0m\n\u001b[1;32m    252\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39maction_logits)\n\u001b[1;32m    253\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m--> 254\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Take step in environment\u001b[39;00m\n\u001b[1;32m    257\u001b[0m next_state, step_reward, done, _ \u001b[38;5;241m=\u001b[39m maze_env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/distributions/categorical.py:141\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    139\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[1;32m    140\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlog_pmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "\n",
    "'''\n",
    "ExperienceBuffer: For storing experiences for reward model training\n",
    "GroupBuffer: For storing and calculating relative advantages between groups of trajectories\n",
    "GRPORewardNetwork: Neural network for predicting rewards in GRPO\n",
    "GRPONetwork: Neural network for policy in GRPO\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Group Buffer for storing policies\n",
    "class GroupBuffer:\n",
    "    def __init__(self, max_size=5):\n",
    "        self.max_size = max_size\n",
    "        self.policies = []\n",
    "        self.returns = []\n",
    "        \n",
    "    def add(self, policy, avg_return):\n",
    "        if len(self.policies) >= self.max_size:\n",
    "            self.policies.pop(0)\n",
    "            self.returns.pop(0)\n",
    "        self.policies.append(policy)\n",
    "        self.returns.append(avg_return)\n",
    "    \n",
    "    def calculate_relative_advantage(self, rewards):\n",
    "        \"\"\"Calculate advantages relative to group performance\"\"\"\n",
    "        if not rewards:\n",
    "            return []\n",
    "        group_mean = np.mean(rewards)\n",
    "        group_std = np.std(rewards) + 1e-8\n",
    "        return (np.array(rewards) - group_mean) / group_std\n",
    "    \n",
    "    def mean_return(self):\n",
    "        return sum(self.returns) / len(self.returns) if self.returns else 0\n",
    "\n",
    "\n",
    "\n",
    "# GRPO Network\n",
    "class GRPONetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        # For maze: obs_dim will be 4 (x, y, target_x, target_y)\n",
    "        # act_dim will be 4 (up, down, left, right)\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.actor(x)\n",
    "\n",
    "\n",
    "# Reward model training buffer\n",
    "class ExperienceBuffer:\n",
    "    \"\"\"Buffer for storing trajectories for reward model training\"\"\"\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.max_size = max_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def add(self, state, action, reward):\n",
    "        if len(self.states) >= self.max_size:\n",
    "            self.states.pop(0)\n",
    "            self.actions.pop(0)\n",
    "            self.rewards.pop(0)\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.states), batch_size, replace=False)\n",
    "        states = torch.stack([self.states[i] for i in indices])\n",
    "        actions = torch.stack([self.actions[i] for i in indices])\n",
    "        rewards = torch.FloatTensor([self.rewards[i] for i in indices])\n",
    "        return states, actions, rewards\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "# Reward network for GRPO\n",
    "class GRPORewardNetwork(nn.Module):\n",
    "    \"\"\"Neural network for predicting rewards in GRPO\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.act_dim = act_dim  # Store action dimension\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        # Create one-hot tensor on the same device as input tensors\n",
    "        action_onehot = torch.zeros(action.size(0), self.act_dim, device=action.device)\n",
    "        action_onehot.scatter_(1, action.unsqueeze(1), 1)\n",
    "        # Concatenate and pass through network\n",
    "        x = torch.cat([state, action_onehot], dim=1)\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# Calculate KL divergence between two policies\n",
    "def train_maze_grpo(maze_env, num_episodes=5000, group_size=64):\n",
    "    \"\"\"\n",
    "    Train an agent using Group Relative Policy Optimization (GRPO)\n",
    "    with proper episode completion handling and improved trajectory collection\n",
    "    \n",
    "    Args:\n",
    "        maze_env: Maze environment instance\n",
    "        num_episodes: Maximum number of episodes to train\n",
    "        group_size: Number of trajectories to collect before updating policy\n",
    "    \"\"\"\n",
    "    # Initialize Pygame for visualization\n",
    "    pygame.init()\n",
    "    SCALE_FACTOR = 3\n",
    "    WINSIZE = (maze_env.w * 16 * SCALE_FACTOR, maze_env.h * 16 * SCALE_FACTOR)\n",
    "    screen = pygame.display.set_mode(WINSIZE)\n",
    "    pygame.display.set_caption('Maze GRPO Training')\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    # Set up TensorBoard logging\n",
    "    writer = SummaryWriter(log_dir=\"./runs/maze_training\")\n",
    "    \n",
    "    # Initialize GRPO components\n",
    "    obs_dim = 4  # State dimensions: (x, y, target_x, target_y)\n",
    "    act_dim = 4  # Action space: (up, down, left, right)\n",
    "\n",
    "    # Set up device (GPU/MPS if available, else CPU)\n",
    "    device = (\"mps\" if torch.backends.mps.is_available() else\n",
    "             \"cuda\" if torch.cuda.is_available() else\n",
    "             \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize neural networks\n",
    "    policy = GRPONetwork(obs_dim, act_dim).to(device)\n",
    "    reference_policy = GRPONetwork(obs_dim, act_dim).to(device)\n",
    "    reward_network = GRPORewardNetwork(obs_dim, act_dim).to(device)\n",
    "    reference_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # Setup optimizers and buffers\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "    reward_optimizer = optim.Adam(reward_network.parameters(), lr=2e-5)\n",
    "    group_buffer = GroupBuffer(max_size=5)\n",
    "    experience_buffer = ExperienceBuffer(max_size=100000)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    gamma = 0.99        # Discount factor for future rewards\n",
    "    epsilon = 0.2       # PPO clipping parameter\n",
    "    beta = 0.04         # KL divergence coefficient\n",
    "    max_steps = 1000    # Maximum steps per episode\n",
    "    num_iterations = 3  # Number of major iterations\n",
    "    episodes_per_iter = num_episodes // num_iterations\n",
    "\n",
    "    # Setup tracking variables\n",
    "    best_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    save_dir = \"saved_models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize font for display\n",
    "    font = pygame.font.Font(None, 36)\n",
    "    \n",
    "    # Training statistics\n",
    "    successful_episodes = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    # Main training loop over iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nStarting Iteration {iteration + 1}/{num_iterations}\")\n",
    "        \n",
    "        # Set reference policy to current policy at start of iteration\n",
    "        reference_policy.load_state_dict(policy.state_dict())\n",
    "        \n",
    "        episode = iteration * episodes_per_iter\n",
    "        running = True\n",
    "        \n",
    "        while running and episode < (iteration + 1) * episodes_per_iter:\n",
    "            # Handle Pygame events\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    break\n",
    "                    \n",
    "            print(f\"\\nStarting Episode {episode + 1}/{num_episodes}\")\n",
    "            \n",
    "            # Initialize group collection variables\n",
    "            group_states = []\n",
    "            group_actions = []\n",
    "            group_rewards = []\n",
    "            group_log_probs = []\n",
    "            group_total_rewards = []\n",
    "            group_successes = 0\n",
    "            \n",
    "            # Collect group of trajectories\n",
    "            valid_trajectories = 0\n",
    "            group_attempts = 0\n",
    "            max_group_attempts = group_size * 2  # Allow some retry attempts\n",
    "            \n",
    "            while valid_trajectories < group_size and group_attempts < max_group_attempts:\n",
    "                # Initialize trajectory variables\n",
    "                states, actions, log_probs, rewards = [], [], [], []\n",
    "                state = maze_env.reset()\n",
    "                total_reward = 0\n",
    "                steps = 0\n",
    "                episode_complete = False\n",
    "                \n",
    "                # Single trajectory collection loop\n",
    "                while not episode_complete and steps < max_steps:\n",
    "                    # Clear screen and draw current state\n",
    "                    screen.fill((0, 0, 0))\n",
    "                    maze_env.draw(screen)\n",
    "                    \n",
    "                    # Display current status\n",
    "                    texts = [\n",
    "                        f'Episode: {episode + 1}/{num_episodes}',\n",
    "                        f'Valid Trajectories: {valid_trajectories}/{group_size}',\n",
    "                        f'Steps: {steps}',\n",
    "                        f'Total Reward: {total_reward:.1f}',\n",
    "                        f'Best Reward: {best_reward:.1f}',\n",
    "                        f'Success Rate: {(successful_episodes/max(1, episode)):.2%}'\n",
    "                    ]\n",
    "                    \n",
    "                    # Render status texts\n",
    "                    for i, text in enumerate(texts):\n",
    "                        text_surface = font.render(text, True, (255, 255, 255))\n",
    "                        screen.blit(text_surface, (10, 10 + i * 30))\n",
    "                    \n",
    "                    pygame.display.flip()\n",
    "                    clock.tick(60)\n",
    "\n",
    "                    # Convert state to tensor and get action from policy\n",
    "                    state_tensor = torch.FloatTensor(state).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        action_logits = policy(state_tensor)\n",
    "                        dist = Categorical(logits=action_logits)\n",
    "                        action = dist.sample()\n",
    "                        log_prob = dist.log_prob(action)\n",
    "                    \n",
    "                    # Take step in environment\n",
    "                    next_state, step_reward, done, _ = maze_env.step(action.item())\n",
    "                    \n",
    "                    # Get reward from reward network\n",
    "                    with torch.no_grad():\n",
    "                        model_reward = reward_network(state_tensor, action.unsqueeze(0))\n",
    "                    \n",
    "                    # Store experience for reward model training\n",
    "                    experience_buffer.add(state_tensor, action, step_reward)\n",
    "                    \n",
    "                    # Store transition\n",
    "                    states.append(state)\n",
    "                    actions.append(action.item())\n",
    "                    log_probs.append(log_prob.item())\n",
    "                    \n",
    "                    # Check win condition and update reward\n",
    "                    if maze_env.check_win():\n",
    "                        reward = 1000.0  # Ensure win reward is given\n",
    "                        rewards.append(reward)\n",
    "                        total_reward += reward\n",
    "                        print(f\"  Trajectory {valid_trajectories + 1}: Success! Steps = {steps}, Final Reward = {total_reward:.2f}\")\n",
    "                        episode_complete = True\n",
    "                        group_successes += 1\n",
    "                    else:\n",
    "                        # Use reward from model and add step bonus\n",
    "                        step_reward = model_reward.item() + 0.1\n",
    "                        rewards.append(step_reward)\n",
    "                        total_reward += step_reward\n",
    "                        \n",
    "                        # Check other completion conditions\n",
    "                        if steps >= max_steps:\n",
    "                            print(f\"  Trajectory {valid_trajectories + 1}: Max steps reached. Reward = {total_reward:.2f}\")\n",
    "                            episode_complete = True\n",
    "                        elif done:\n",
    "                            collision_penalty = -0.5\n",
    "                            rewards.append(collision_penalty)\n",
    "                            total_reward += collision_penalty\n",
    "                            episode_complete = True\n",
    "                    \n",
    "                    state = next_state\n",
    "                    steps += 1\n",
    "                \n",
    "                # Store trajectory if complete\n",
    "                if episode_complete:\n",
    "                    group_states.append(states)\n",
    "                    group_actions.append(actions)\n",
    "                    group_rewards.append(rewards)\n",
    "                    group_log_probs.append(log_probs)\n",
    "                    group_total_rewards.append(total_reward)\n",
    "                    valid_trajectories += 1\n",
    "                    total_steps += steps\n",
    "                \n",
    "                group_attempts += 1\n",
    "            \n",
    "            if not running:\n",
    "                break\n",
    "                \n",
    "            # Only proceed with updates if we have collected enough trajectories\n",
    "            if valid_trajectories > 0:\n",
    "                # Update reward network with experiences\n",
    "                if len(experience_buffer) > 1000:  # Minimum size before training\n",
    "                    states_batch, actions_batch, rewards_batch = experience_buffer.sample(256)\n",
    "                    predicted_rewards = reward_network(states_batch, actions_batch)\n",
    "                    reward_loss = F.mse_loss(predicted_rewards, rewards_batch)\n",
    "                    \n",
    "                    reward_optimizer.zero_grad()\n",
    "                    reward_loss.backward()\n",
    "                    reward_optimizer.step()\n",
    "                \n",
    "                # Calculate group-relative advantages\n",
    "                relative_advantages = group_buffer.calculate_relative_advantage(group_total_rewards)\n",
    "                \n",
    "                # Policy update loop\n",
    "                for trajectory_idx in range(valid_trajectories):\n",
    "                    # Convert trajectory data to tensors\n",
    "                    states = torch.FloatTensor(group_states[trajectory_idx]).to(device)\n",
    "                    actions = torch.LongTensor(group_actions[trajectory_idx]).to(device)\n",
    "                    old_log_probs = torch.FloatTensor(group_log_probs[trajectory_idx]).to(device)\n",
    "                    advantage = torch.FloatTensor([relative_advantages[trajectory_idx]] * len(states)).to(device)\n",
    "                    \n",
    "                    # Calculate policy loss\n",
    "                    action_logits = policy(states)\n",
    "                    dist = Categorical(logits=action_logits)\n",
    "                    new_log_probs = dist.log_prob(actions)\n",
    "                    \n",
    "                    # Calculate PPO ratios and surrogate loss\n",
    "                    ratio = (new_log_probs - old_log_probs).exp()\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                    \n",
    "                    # Calculate KL divergence loss\n",
    "                    kl_loss = beta * calculate_kl_divergence(policy, reference_policy, states).mean()\n",
    "                    \n",
    "                    # Compute total loss and update policy\n",
    "                    total_loss = policy_loss + kl_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Update group buffer and track rewards\n",
    "                avg_reward = np.mean(group_total_rewards)\n",
    "                episode_rewards.append(avg_reward)\n",
    "                group_buffer.add(policy.state_dict(), avg_reward)\n",
    "                \n",
    "                # Update success statistics\n",
    "                successful_episodes += (group_successes > 0)\n",
    "                \n",
    "                # Save best model\n",
    "                if avg_reward > best_reward:\n",
    "                    best_reward = avg_reward\n",
    "                    torch.save({\n",
    "                        'iteration': iteration,\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': policy.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'reward_model_state_dict': reward_network.state_dict(),\n",
    "                        'reward': avg_reward,\n",
    "                        'steps': total_steps,\n",
    "                        'successes': successful_episodes,\n",
    "                    }, os.path.join(save_dir, f'best_model_iter_{iteration}.pt'))\n",
    "                    print(f\"\\nNew best model saved! Reward: {avg_reward:.2f}\")\n",
    "                \n",
    "                # Logging\n",
    "                if episode % 10 == 0:\n",
    "                    writer.add_scalar(\"Training/Average_Reward\", avg_reward, episode)\n",
    "                    writer.add_scalar(\"Training/Best_Reward\", best_reward, episode)\n",
    "                    writer.add_scalar(\"Training/Success_Rate\", successful_episodes/(episode+1), episode)\n",
    "                    writer.add_scalar(\"Training/Average_Steps\", total_steps/(episode+1), episode)\n",
    "                    writer.add_scalar(\"Training/Reward_Loss\", reward_loss.item(), episode)\n",
    "                    \n",
    "                    avg_last_100 = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "                    print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "                    print(f\"  Iteration: {iteration + 1}/{num_iterations}\")\n",
    "                    print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "                    print(f\"  Best Reward: {best_reward:.2f}\")\n",
    "                    print(f\"  Success Rate: {(successful_episodes/(episode+1)):.2%}\")\n",
    "                    print(f\"  Average Steps per Episode: {(total_steps/(episode+1)):.1f}\")\n",
    "                    print(f\"  Last 100 Episodes Average: {avg_last_100:.2f}\")\n",
    "                    print(f\"  Reward Model Loss: {reward_loss.item():.4f}\")\n",
    "            \n",
    "            # Update episode counter\n",
    "            episode += 1\n",
    "            pygame.time.wait(100)  # Reduced delay between episodes for faster training\n",
    "        \n",
    "    # Final cleanup\n",
    "    pygame.quit()\n",
    "    writer.close()\n",
    "    \n",
    "    # Save final models\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'model_state_dict': policy.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'reward_model_state_dict': reward_network.state_dict(),\n",
    "        'reward': avg_reward if 'avg_reward' in locals() else 0,\n",
    "        'steps': total_steps,\n",
    "        'successes': successful_episodes,\n",
    "    }, os.path.join(save_dir, 'final_model.pt'))\n",
    "    \n",
    "    return policy, reward_network\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from maze_environment import Maze  \n",
    "\n",
    "    # Create maze environment\n",
    "    maze_env = Maze(level=0)  # Start with smallest maze\n",
    "\n",
    "    # Train the agent\n",
    "    policy = train_maze_grpo(maze_env, num_episodes=5000)\n",
    "\n",
    "    # Save the trained policy\n",
    "    torch.save(policy.state_dict(), \"maze_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
